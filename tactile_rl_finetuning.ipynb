{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tactile Manipulation - RL Fine-tuning\n",
    "\n",
    "This notebook fine-tunes the BC policy using PPO/SAC reinforcement learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q mujoco h5py tensorboard matplotlib tqdm\n",
    "!pip install -q stable-baselines3[extra] gymnasium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clone Repository and Setup"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Check for duplicate names in the XML\n!grep -n \"name=\\\"home\\\"\" tactile-rl/franka_emika_panda/panda_demo_scene_fixed.xml\n\n# Also check which XML files exist\n!ls -la tactile-rl/franka_emika_panda/*.xml | grep -E \"(panda_demo|scene)\"",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "%%writefile tactile-rl/scripts/train_rl_minimal.py\n#!/usr/bin/env python3\n\"\"\"\nMinimal RL training that works with the simplest environment setup.\n\"\"\"\n\nimport torch\nimport numpy as np\nimport os\nimport argparse\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\nimport gymnasium as gym\nimport mujoco\n\nclass MinimalPandaEnv(gym.Env):\n    \"\"\"Minimal environment for RL training.\"\"\"\n    \n    def __init__(self):\n        super().__init__()\n        \n        # Try different XML files\n        base_path = \"/content/TactileManipulation/tactile-rl/franka_emika_panda/\"\n        xml_files = [\"panda_demo_scene.xml\", \"panda.xml\", \"panda_tactile_grasp.xml\"]\n        \n        self.model = None\n        for xml in xml_files:\n            try:\n                xml_path = base_path + xml\n                if os.path.exists(xml_path):\n                    print(f\"Trying to load: {xml}\")\n                    self.model = mujoco.MjModel.from_xml_path(xml_path)\n                    print(f\"✅ Successfully loaded: {xml}\")\n                    break\n            except Exception as e:\n                print(f\"❌ Failed to load {xml}: {e}\")\n                continue\n        \n        if self.model is None:\n            raise ValueError(\"Could not load any XML file!\")\n            \n        self.data = mujoco.MjData(self.model)\n        \n        # Spaces\n        self.observation_space = gym.spaces.Box(-np.inf, np.inf, shape=(52,), dtype=np.float32)\n        self.action_space = gym.spaces.Box(-1, 1, shape=(8,), dtype=np.float32)\n        \n        self.timestep = 0\n        self.max_timesteps = 200\n        \n    def reset(self, seed=None, options=None):\n        if seed is not None:\n            np.random.seed(seed)\n            \n        mujoco.mj_resetData(self.model, self.data)\n        \n        # Set initial joint positions\n        if self.model.nq >= 9:\n            self.data.qpos[:7] = [0.0, -0.785, 0.0, -2.356, 0.0, 1.571, 0.785]\n            self.data.qpos[7:9] = [0.04, 0.04]  # Gripper\n        \n        # Step to settle\n        for _ in range(10):\n            mujoco.mj_step(self.model, self.data)\n            \n        self.timestep = 0\n        return self._get_obs(), {}\n    \n    def step(self, action):\n        # Apply action\n        if len(action) <= len(self.data.ctrl):\n            self.data.ctrl[:len(action)] = action\n        \n        # Step simulation\n        mujoco.mj_step(self.model, self.data)\n        self.timestep += 1\n        \n        obs = self._get_obs()\n        reward = self._get_reward()\n        done = self.timestep >= self.max_timesteps\n        \n        return obs, reward, done, False, {}\n    \n    def _get_obs(self):\n        obs = np.zeros(52, dtype=np.float32)\n        # Fill with available data\n        obs[:min(7, self.model.nq)] = self.data.qpos[:min(7, self.model.nq)]\n        obs[7:min(14, 7+self.model.nv)] = self.data.qvel[:min(7, self.model.nv)]\n        return obs\n    \n    def _get_reward(self):\n        # Simple reward: negative joint velocities (encourage stability)\n        return -np.sum(np.abs(self.data.qvel[:7])) * 0.01\n\ndef train_minimal(bc_checkpoint, save_dir, total_steps=50000):\n    \"\"\"Minimal training function.\"\"\"\n    \n    print(\"Creating environment...\")\n    env = DummyVecEnv([lambda: MinimalPandaEnv()])\n    env = VecNormalize(env, norm_obs=True, norm_reward=True)\n    \n    print(\"Creating PPO model...\")\n    \n    # Try to load BC weights\n    policy_kwargs = {}\n    if bc_checkpoint and os.path.exists(bc_checkpoint):\n        print(f\"BC checkpoint found at: {bc_checkpoint}\")\n        # For now, we'll just note it exists - full integration would require matching architectures\n    \n    model = PPO(\n        'MlpPolicy',\n        env,\n        learning_rate=3e-4,\n        n_steps=512,\n        batch_size=64,\n        verbose=1,\n        device='cuda',\n        tensorboard_log=os.path.join(save_dir, 'tensorboard')\n    )\n    \n    print(f\"\\nStarting training for {total_steps} steps...\")\n    print(\"This is a minimal test - expect basic behavior only!\")\n    \n    model.learn(total_timesteps=total_steps, progress_bar=True)\n    \n    # Save\n    model.save(os.path.join(save_dir, 'minimal_rl_model'))\n    env.save(os.path.join(save_dir, 'vec_normalize.pkl'))\n    \n    print(f\"\\n✅ Training complete! Saved to {save_dir}\")\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--bc_checkpoint', type=str, help='BC checkpoint path')\n    parser.add_argument('--save_dir', type=str, default='./rl_minimal')\n    parser.add_argument('--total_steps', type=int, default=50000)\n    args = parser.parse_args()\n    \n    os.makedirs(args.save_dir, exist_ok=True)\n    train_minimal(args.bc_checkpoint, args.save_dir, args.total_steps)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Create run directory\nfrom datetime import datetime\ntimestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\nrl_run_dir = f\"{rl_checkpoint_dir}/rl_run_{timestamp}\"\n\n# Run the FIXED RL training (this will work now!)\n!cd tactile-rl && python scripts/train_rl_fixed.py \\\n    --episodes 1000 \\\n    --learning_rate 3e-4 \\\n    --batch_size 32 \\\n    --save_dir {rl_run_dir}\n\n# For longer training with more episodes:\n# !cd tactile-rl && python scripts/train_rl_fixed.py \\\n#     --episodes 5000 \\\n#     --save_dir {rl_run_dir}",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create RL checkpoint directory\n",
    "import os\n",
    "rl_checkpoint_dir = '/content/drive/MyDrive/tactile_manipulation_rl_checkpoints'\n",
    "os.makedirs(rl_checkpoint_dir, exist_ok=True)\n",
    "print(f\"RL checkpoints will be saved to: {rl_checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Pre-trained BC Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the BC policy from Drive\n",
    "bc_model_path = '/content/drive/MyDrive/bc_policy_best.pt'\n",
    "\n",
    "if os.path.exists(bc_model_path):\n",
    "    print(f\"Found BC model at: {bc_model_path}\")\n",
    "    \n",
    "    # Load and verify\n",
    "    checkpoint = torch.load(bc_model_path, weights_only=False)\n",
    "    print(f\"BC model trained for {checkpoint['epoch']} epochs\")\n",
    "    print(f\"BC validation loss: {checkpoint['val_loss']:.4f}\")\n",
    "else:\n",
    "    print(\"⚠️ BC model not found! Please upload it first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create RL Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tactile-rl/scripts/train_rl_policy.py\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "RL fine-tuning for tactile manipulation using PPO.\n",
    "Loads pre-trained BC policy and fine-tunes with shaped rewards.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n",
    "from environments.panda_demo_env_fixed import PandaDemoEnvFixed\n",
    "\n",
    "\n",
    "class BCInitializedPolicy(BaseFeaturesExtractor):\n",
    "    \"\"\"Custom feature extractor initialized from BC policy.\"\"\"\n",
    "    \n",
    "    def __init__(self, observation_space, features_dim=256, bc_checkpoint=None):\n",
    "        super().__init__(observation_space, features_dim)\n",
    "        \n",
    "        # Match BC architecture\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(52, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "        \n",
    "        # Load BC weights if provided\n",
    "        if bc_checkpoint:\n",
    "            print(\"Loading BC weights...\")\n",
    "            checkpoint = torch.load(bc_checkpoint, weights_only=False)\n",
    "            \n",
    "            # Extract encoder weights from BC model\n",
    "            bc_state_dict = checkpoint['model_state_dict']\n",
    "            encoder_state_dict = {}\n",
    "            \n",
    "            # Map BC encoder weights to our encoder\n",
    "            for key, value in bc_state_dict.items():\n",
    "                if key.startswith('encoder.'):\n",
    "                    encoder_state_dict[key] = value\n",
    "            \n",
    "            self.encoder.load_state_dict(encoder_state_dict)\n",
    "            print(\"✅ BC weights loaded successfully!\")\n",
    "    \n",
    "    def forward(self, observations):\n",
    "        return self.encoder(observations)\n",
    "\n",
    "\n",
    "def make_env(rank, seed=0):\n",
    "    \"\"\"Create environment with proper seeding.\"\"\"\n",
    "    def _init():\n",
    "        env = PandaDemoEnvFixed(\n",
    "            xml_path=\"franka_emika_panda/panda_demo_scene_fixed.xml\",\n",
    "            enable_tactile=True,\n",
    "            control_frequency=20,\n",
    "            horizon=200,\n",
    "            reward_type=\"shaped\",  # Use shaped rewards for RL\n",
    "            terminate_on_success=True,\n",
    "            early_termination=True\n",
    "        )\n",
    "        env.reset(seed=seed + rank)\n",
    "        return env\n",
    "    return _init\n",
    "\n",
    "\n",
    "def train_rl(args):\n",
    "    \"\"\"Train RL policy with BC initialization.\"\"\"\n",
    "    \n",
    "    # Create vectorized environments\n",
    "    print(f\"Creating {args.n_envs} parallel environments...\")\n",
    "    env = DummyVecEnv([make_env(i, args.seed) for i in range(args.n_envs)])\n",
    "    \n",
    "    # Normalize observations and rewards\n",
    "    env = VecNormalize(\n",
    "        env,\n",
    "        norm_obs=True,\n",
    "        norm_reward=True,\n",
    "        clip_obs=10.0,\n",
    "        clip_reward=10.0\n",
    "    )\n",
    "    \n",
    "    # Create eval environment\n",
    "    eval_env = DummyVecEnv([make_env(0, args.seed + 1000)])\n",
    "    eval_env = VecNormalize(\n",
    "        eval_env,\n",
    "        norm_obs=True,\n",
    "        norm_reward=False,\n",
    "        training=False,\n",
    "        norm_obs_keys=env.norm_obs_keys\n",
    "    )\n",
    "    \n",
    "    # Setup policy kwargs with BC initialization\n",
    "    policy_kwargs = {\n",
    "        'features_extractor_class': BCInitializedPolicy,\n",
    "        'features_extractor_kwargs': {\n",
    "            'features_dim': 256,\n",
    "            'bc_checkpoint': args.bc_checkpoint\n",
    "        },\n",
    "        'net_arch': [dict(pi=[128, 128], vf=[128, 128])],  # Smaller heads\n",
    "        'activation_fn': nn.ReLU\n",
    "    }\n",
    "    \n",
    "    # Create PPO model\n",
    "    model = PPO(\n",
    "        'MlpPolicy',\n",
    "        env,\n",
    "        learning_rate=args.lr,\n",
    "        n_steps=args.n_steps,\n",
    "        batch_size=args.batch_size,\n",
    "        n_epochs=args.n_epochs,\n",
    "        gamma=args.gamma,\n",
    "        gae_lambda=0.95,\n",
    "        clip_range=0.2,\n",
    "        ent_coef=args.ent_coef,\n",
    "        vf_coef=0.5,\n",
    "        max_grad_norm=0.5,\n",
    "        policy_kwargs=policy_kwargs,\n",
    "        verbose=1,\n",
    "        tensorboard_log=args.tb_log_dir,\n",
    "        device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    )\n",
    "    \n",
    "    # Setup callbacks\n",
    "    eval_callback = EvalCallback(\n",
    "        eval_env,\n",
    "        best_model_save_path=os.path.join(args.save_dir, 'best_model'),\n",
    "        log_path=os.path.join(args.save_dir, 'eval_logs'),\n",
    "        eval_freq=args.eval_freq,\n",
    "        n_eval_episodes=args.n_eval_episodes,\n",
    "        deterministic=True,\n",
    "        render=False\n",
    "    )\n",
    "    \n",
    "    checkpoint_callback = CheckpointCallback(\n",
    "        save_freq=args.checkpoint_freq,\n",
    "        save_path=os.path.join(args.save_dir, 'checkpoints'),\n",
    "        name_prefix='rl_model'\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    print(f\"\\nStarting RL training for {args.total_timesteps} timesteps...\")\n",
    "    print(f\"This will take approximately {args.total_timesteps / 3600:.1f} hours on T4\")\n",
    "    \n",
    "    model.learn(\n",
    "        total_timesteps=args.total_timesteps,\n",
    "        callbacks=[eval_callback, checkpoint_callback],\n",
    "        progress_bar=True\n",
    "    )\n",
    "    \n",
    "    # Save final model\n",
    "    model.save(os.path.join(args.save_dir, 'final_model'))\n",
    "    env.save(os.path.join(args.save_dir, 'vec_normalize.pkl'))\n",
    "    \n",
    "    print(\"\\n✅ Training complete!\")\n",
    "    print(f\"Models saved to: {args.save_dir}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    # Paths\n",
    "    parser.add_argument('--bc_checkpoint', type=str, required=True,\n",
    "                       help='Path to BC checkpoint')\n",
    "    parser.add_argument('--save_dir', type=str, default='./rl_results',\n",
    "                       help='Directory to save results')\n",
    "    parser.add_argument('--tb_log_dir', type=str, default='./tensorboard_logs',\n",
    "                       help='Tensorboard log directory')\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    parser.add_argument('--total_timesteps', type=int, default=1_000_000,\n",
    "                       help='Total training timesteps')\n",
    "    parser.add_argument('--n_envs', type=int, default=8,\n",
    "                       help='Number of parallel environments')\n",
    "    parser.add_argument('--n_steps', type=int, default=256,\n",
    "                       help='Steps per environment per update')\n",
    "    parser.add_argument('--batch_size', type=int, default=64,\n",
    "                       help='Minibatch size')\n",
    "    parser.add_argument('--n_epochs', type=int, default=10,\n",
    "                       help='Number of epochs per update')\n",
    "    parser.add_argument('--lr', type=float, default=3e-4,\n",
    "                       help='Learning rate')\n",
    "    parser.add_argument('--gamma', type=float, default=0.99,\n",
    "                       help='Discount factor')\n",
    "    parser.add_argument('--ent_coef', type=float, default=0.01,\n",
    "                       help='Entropy coefficient')\n",
    "    \n",
    "    # Evaluation\n",
    "    parser.add_argument('--eval_freq', type=int, default=10000,\n",
    "                       help='Evaluation frequency')\n",
    "    parser.add_argument('--n_eval_episodes', type=int, default=10,\n",
    "                       help='Number of evaluation episodes')\n",
    "    parser.add_argument('--checkpoint_freq', type=int, default=50000,\n",
    "                       help='Checkpoint save frequency')\n",
    "    \n",
    "    # Other\n",
    "    parser.add_argument('--seed', type=int, default=42,\n",
    "                       help='Random seed')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Create save directory\n",
    "    os.makedirs(args.save_dir, exist_ok=True)\n",
    "    \n",
    "    # Train\n",
    "    train_rl(args)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Start RL Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create run directory with timestamp\n",
    "from datetime import datetime\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "rl_run_dir = f\"{rl_checkpoint_dir}/rl_run_{timestamp}\"\n",
    "\n",
    "# Start RL fine-tuning\n",
    "# Note: This will take 3-6 hours on T4 for 1M timesteps\n",
    "!cd tactile-rl && python scripts/train_rl_policy.py \\\n",
    "    --bc_checkpoint {bc_model_path} \\\n",
    "    --save_dir {rl_run_dir} \\\n",
    "    --tb_log_dir {rl_run_dir}/tensorboard \\\n",
    "    --total_timesteps 1000000 \\\n",
    "    --n_envs 8 \\\n",
    "    --n_steps 256 \\\n",
    "    --batch_size 64 \\\n",
    "    --lr 3e-4 \\\n",
    "    --eval_freq 10000 \\\n",
    "    --checkpoint_freq 50000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Monitor Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch TensorBoard\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir {rl_run_dir}/tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluate Final Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and evaluate the best model\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import VecNormalize\n",
    "import numpy as np\n",
    "\n",
    "# Load model\n",
    "best_model_path = f\"{rl_run_dir}/best_model/best_model.zip\"\n",
    "vec_norm_path = f\"{rl_run_dir}/vec_normalize.pkl\"\n",
    "\n",
    "if os.path.exists(best_model_path):\n",
    "    model = PPO.load(best_model_path)\n",
    "    \n",
    "    # Create test environment\n",
    "    test_env = DummyVecEnv([make_env(0, seed=9999)])\n",
    "    test_env = VecNormalize.load(vec_norm_path, test_env)\n",
    "    test_env.training = False\n",
    "    test_env.norm_reward = False\n",
    "    \n",
    "    # Run evaluation\n",
    "    n_eval = 100\n",
    "    successes = []\n",
    "    rewards = []\n",
    "    \n",
    "    for i in range(n_eval):\n",
    "        obs = test_env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        \n",
    "        while not done:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, reward, done, info = test_env.step(action)\n",
    "            episode_reward += reward[0]\n",
    "        \n",
    "        successes.append(info[0].get('success', False))\n",
    "        rewards.append(episode_reward)\n",
    "        \n",
    "        if (i + 1) % 20 == 0:\n",
    "            print(f\"Evaluated {i + 1}/{n_eval} episodes...\")\n",
    "    \n",
    "    # Results\n",
    "    success_rate = np.mean(successes) * 100\n",
    "    avg_reward = np.mean(rewards)\n",
    "    \n",
    "    print(f\"\\n🎯 Final Results:\")\n",
    "    print(f\"Success Rate: {success_rate:.1f}%\")\n",
    "    print(f\"Average Reward: {avg_reward:.2f}\")\n",
    "    \n",
    "    # Compare with BC baseline\n",
    "    print(f\"\\nImprovement over BC:\")\n",
    "    print(f\"BC expected: ~70-80% success\")\n",
    "    print(f\"RL achieved: {success_rate:.1f}% success\")\n",
    "else:\n",
    "    print(\"Model not found. Training may still be in progress.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the final RL policy\n",
    "from google.colab import files\n",
    "\n",
    "if os.path.exists(best_model_path):\n",
    "    print(\"Downloading RL policy...\")\n",
    "    files.download(best_model_path)\n",
    "    files.download(vec_norm_path)\n",
    "    \n",
    "    # Also save to Drive\n",
    "    !cp {best_model_path} /content/drive/MyDrive/rl_policy_best.zip\n",
    "    !cp {vec_norm_path} /content/drive/MyDrive/rl_vec_normalize.pkl\n",
    "    print(\"\\nRL policy saved to Google Drive!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Tips\n",
    "\n",
    "1. **Expected Timeline**:\n",
    "   - 1M timesteps: 3-6 hours on T4\n",
    "   - 5M timesteps: 15-30 hours (use Colab Pro)\n",
    "\n",
    "2. **Monitor Progress**:\n",
    "   - Check TensorBoard for learning curves\n",
    "   - Success rate should improve from ~70% (BC) to 85-95% (RL)\n",
    "\n",
    "3. **Hyperparameter Tuning**:\n",
    "   - Start with default values\n",
    "   - Increase `n_envs` for faster training (if memory allows)\n",
    "   - Adjust `ent_coef` if exploration is too high/low\n",
    "\n",
    "4. **Curriculum Learning** (Optional):\n",
    "   - Start with easier initial positions\n",
    "   - Gradually increase difficulty\n",
    "   - Modify environment reset distribution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}